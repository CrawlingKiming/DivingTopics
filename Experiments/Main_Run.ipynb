{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import collections \n",
    "import time\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "import re \n",
    "#from eunjeon import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"bow_corpus\", \"rb\") as f : \n",
    "#    bow_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(bow_corpus) == 777832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 전처리가 된 excel은 pre_processing이 아니라 read_token을 실행하면 된다. \n",
    "def read_token(text):\n",
    "    text = str(text)\n",
    "    result = text.split()\n",
    "    result = [i for i in result if type(i) == str]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = [\"processed_SNS_1.xlsx\",\"processed_SNS_2.xlsx\",\"processed_SNS_3.xlsx\",\"processed_SNS_4.xlsx\",\"processed_SNS_5.xlsx\", \"processed_SNS_6.xlsx\", \"processed_SNS_7.xlsx\", \"processed_SNS_8.xlsx\"]\n",
    "#collected_df\n",
    "collected_df = pd.read_excel(flist[0])\n",
    "#collected_df iteration을 이용하며 총 8개로 분산되어있는 excel파일을 하나의 dataframe으로 가져온다. \n",
    "for i in range(1,len(flist)):\n",
    "    print(collected_df.shape)\n",
    "    temp_df = pd.read_excel(flist[i])\n",
    "    collected_df = collected_df.append(temp_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_token하면 자동으로 doc당 형태소 list를 만들 수 있다. \n",
    "processed_docs = collected_df[\"CONTENT\"].map(read_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tox(processed_docs) :  \n",
    "    #get index_to_words\n",
    "    id2word = gensim.corpora.Dictionary(processed_docs)\n",
    "    #filter_extreme\n",
    "    id2word.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 100000)\n",
    "    id2word.compactify()\n",
    "    \n",
    "    bow_corpus = [id2word.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "    return id2word, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word, bow_corpus = get_tox(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bow_corpus.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"idxlist_shuffled\", \"rb\") as f : \n",
    "    idx_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generator(idx_list, batch_unit = 10):\n",
    "    print(len(idx_list))\n",
    "    batch_index = np.array_split(idx_list, batch_unit)\n",
    "    batch_list = []\n",
    "    for i in range(len(batch_index)) : \n",
    "        temp = [bow_corpus[j] for j in batch_index[i]]\n",
    "        batch_list.append(temp)\n",
    "    print(len(batch_list) == batch_unit)\n",
    "    \n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_feed(batch):\n",
    "    #lda_trained = gensim.models.LdaModel.load(\"lda_trained.model\")\n",
    "    lda_trained.update(batch)\n",
    "    lda_trained.save(\"lda_trained.model\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777832\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "batch_list = stream_generator(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log to external log file\n",
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set up log to terminal\n",
    "#import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_trained =  gensim.models.ldamulticore.LdaMulticore(\n",
    "                       corpus = batch_list[0],\n",
    "                       num_topics=15,\n",
    "                       id2word=id2word,\n",
    "                       workers=3, # Num. Processing Cores - 1\n",
    "                       passes=5,\n",
    "                       chunksize = 100,\n",
    "                       eval_every = 1,\n",
    "                       per_word_topics=True)\n",
    "lda_trained.save('lda_trained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_trained.update(batch_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3th batch is now updating...\n",
      "Done!\n",
      "4th batch is now updating...\n",
      "Done!\n",
      "5th batch is now updating...\n",
      "Done!\n",
      "6th batch is now updating...\n",
      "Done!\n",
      "7th batch is now updating...\n",
      "Done!\n",
      "8th batch is now updating...\n",
      "Done!\n",
      "9th batch is now updating...\n",
      "Done!\n",
      "10th batch is now updating...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,len(batch_list)) :\n",
    "    print(\"{0}th batch is now updating...\".format(i+1))\n",
    "    lda_trained.update(batch_list[i])\n",
    "    print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.055*\"제품\" + 0.044*\"새\" + 0.042*\"남\" + 0.037*\"판매\" + 0.033*\"▶\" + 0.032*\"원\" + 0.031*\"개\" + 0.029*\"가능\" + 0.024*\"비\" + 0.024*\"천\" + 0.022*\"택배\" + 0.021*\"수량\" + 0.021*\"별도\" + 0.020*\"직거래\" + 0.012*\"완료\" + 0.012*\"세트\"\n",
      "Topic: 1 Word: 0.015*\"적\" + 0.011*\"건강\" + 0.009*\"합니다\" + 0.008*\"때문\" + 0.008*\"인\" + 0.008*\"먼지\" + 0.007*\"주\" + 0.007*\"경우\" + 0.006*\"미세\" + 0.006*\"많\" + 0.006*\"치료\" + 0.006*\"발생\" + 0.005*\"성\" + 0.005*\"질환\" + 0.005*\"시\" + 0.005*\"물질\"\n",
      "Topic: 2 Word: 0.013*\"대\" + 0.009*\"성\" + 0.008*\"곳\" + 0.008*\"역\" + 0.007*\"눈\" + 0.006*\"인\" + 0.006*\"피부\" + 0.005*\"없애\" + 0.005*\"코\" + 0.005*\"잘\" + 0.005*\"볼\" + 0.004*\"턱\" + 0.004*\"라인\" + 0.004*\"목\" + 0.004*\"관리\" + 0.004*\"시술\"\n",
      "Topic: 3 Word: 0.038*\"먹\" + 0.015*\"맛있\" + 0.015*\"맛\" + 0.010*\"원\" + 0.009*\"시\" + 0.008*\"카페\" + 0.007*\"맛집\" + 0.007*\"주문\" + 0.006*\"배\" + 0.006*\"주\" + 0.006*\"고기\" + 0.005*\"식물\" + 0.005*\"메뉴\" + 0.005*\"선물\" + 0.005*\"음식\" + 0.005*\"넣\"\n",
      "Topic: 4 Word: 0.011*\"·\" + 0.010*\"환경\" + 0.009*\"적\" + 0.009*\"사업\" + 0.008*\"’\" + 0.007*\"지원\" + 0.007*\"인\" + 0.006*\"원\" + 0.006*\"지역\" + 0.006*\"도시\" + 0.006*\"위해\" + 0.006*\"한다\" + 0.006*\"먼지\" + 0.005*\"계획\" + 0.005*\"“\" + 0.005*\"”\"\n",
      "Topic: 5 Word: 0.031*\"피부\" + 0.017*\"사용\" + 0.013*\"제품\" + 0.013*\"주\" + 0.009*\"t\" + 0.009*\"클렌징\" + 0.008*\"성분\" + 0.007*\"크림\" + 0.006*\"팩\" + 0.006*\"수분\" + 0.005*\"관리\" + 0.005*\"w\" + 0.005*\"e\" + 0.005*\"자극\" + 0.005*\"인\" + 0.005*\"더\"\n",
      "Topic: 6 Word: 0.017*\"는데\" + 0.011*\"같\" + 0.011*\"안\" + 0.010*\"거\" + 0.010*\"네요\" + 0.009*\"니\" + 0.009*\"없\" + 0.009*\"ㅋㅋㅋ\" + 0.008*\"주\" + 0.008*\"ㅋㅋ\" + 0.008*\"만\" + 0.008*\"너무\" + 0.007*\"서\" + 0.007*\"던\" + 0.007*\"어서\" + 0.007*\"잘\"\n",
      "Topic: 7 Word: 0.012*\"원\" + 0.011*\"’\" + 0.010*\"주\" + 0.009*\"·\" + 0.008*\"관련\" + 0.008*\"억\" + 0.008*\"기술\" + 0.008*\"인\" + 0.007*\"만\" + 0.007*\"기업\" + 0.007*\"개발\" + 0.006*\"시장\" + 0.006*\"투자\" + 0.005*\"국내\" + 0.005*\"화\" + 0.004*\"사업\"\n",
      "Topic: 8 Word: 0.015*\"적\" + 0.012*\"“\" + 0.012*\"”\" + 0.008*\"인\" + 0.007*\"없\" + 0.007*\"’\" + 0.007*\"만\" + 0.007*\"다고\" + 0.006*\"다는\" + 0.006*\"문제\" + 0.006*\"라고\" + 0.006*\"정부\" + 0.006*\"·\" + 0.006*\"말\" + 0.005*\"중국\" + 0.005*\"며\"\n",
      "Topic: 9 Word: 0.008*\"며\" + 0.007*\"인\" + 0.006*\"길\" + 0.006*\"사람\" + 0.005*\"사진\" + 0.005*\"만\" + 0.005*\"던\" + 0.005*\"적\" + 0.005*\"여행\" + 0.004*\"한다\" + 0.004*\"합니다\" + 0.004*\"체험\" + 0.004*\"주\" + 0.004*\"말\" + 0.004*\"곳\" + 0.003*\"없\"\n",
      "Topic: 10 Word: 0.030*\"시\" + 0.017*\"원\" + 0.016*\"합니다\" + 0.016*\"만\" + 0.015*\"차량\" + 0.014*\"세요\" + 0.012*\"주\" + 0.008*\"차\" + 0.007*\"자동차\" + 0.007*\"받\" + 0.006*\"구매\" + 0.006*\"가능\" + 0.006*\"판매\" + 0.006*\"님\" + 0.006*\"확인\" + 0.005*\"경우\"\n",
      "Topic: 11 Word: 0.018*\"연석\" + 0.015*\"살\" + 0.010*\"법\" + 0.010*\"역\" + 0.010*\"대\" + 0.008*\"약\" + 0.008*\"다이어트\" + 0.006*\"블럭\" + 0.006*\"인\" + 0.006*\"루\" + 0.006*\"장당\" + 0.005*\"주\" + 0.005*\"성\" + 0.005*\"적\" + 0.005*\"빼\" + 0.004*\"주일\"\n",
      "Topic: 12 Word: 0.013*\"시\" + 0.011*\"시공\" + 0.008*\"층\" + 0.008*\"공간\" + 0.007*\"아파트\" + 0.007*\"설치\" + 0.006*\"합니다\" + 0.006*\"방충망\" + 0.006*\"지도\" + 0.006*\"빌라\" + 0.006*\"저장\" + 0.006*\"닫\" + 0.005*\"없\" + 0.005*\"실\" + 0.005*\"가능\" + 0.005*\"위치\"\n",
      "Topic: 13 Word: 0.020*\"기온\" + 0.015*\"·\" + 0.014*\"서울\" + 0.014*\"낮\" + 0.013*\"전국\" + 0.011*\"농도\" + 0.011*\"지역\" + 0.010*\"비\" + 0.010*\"오늘\" + 0.010*\"날씨\" + 0.009*\"나쁨\" + 0.009*\"먼지\" + 0.009*\"∼\" + 0.008*\"최고\" + 0.008*\"아침\" + 0.008*\"예상\"\n",
      "Topic: 14 Word: 0.032*\"공기\" + 0.018*\"먼지\" + 0.016*\"청정기\" + 0.015*\"사용\" + 0.015*\"청소\" + 0.013*\"필터\" + 0.010*\"미세\" + 0.009*\"마스크\" + 0.009*\"제품\" + 0.008*\"주\" + 0.008*\"제거\" + 0.006*\"실내\" + 0.006*\"렌탈\" + 0.005*\"케어\" + 0.005*\"에어컨\" + 0.005*\"적\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_trained.print_topics(16, num_words = 17):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
